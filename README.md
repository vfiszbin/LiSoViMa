# MNLP M3 â€” Final Project Submission

This repository contains the complete pipeline, codebase, and configuration files to reproduce and evaluate four fine-tuned language models developed during the Modern Natural Language Processing course project (M3):  
- **MCQA model**
- **Quantized model**
- **Retrieval-Augmented Generation (RAG) model**
- **Direct Preference Optimization (DPO) model**

We also include a pipeline for **Supervised Fine-Tuning (SFT)** on STEM data, which serves as a base for some of the above models.

---

## Repository Structure

```
.
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ train_mcqa/               # Code to fine-tune the MCQA model
â”‚   â”œâ”€â”€ train_quantized/          # Code for quantization-based fine-tuning
â”‚   â”œâ”€â”€ train_rag/                # Code for training the RAG model
â”‚   â”œâ”€â”€ train_dpo/                # Code to train the DPO model
â”‚   â”œâ”€â”€ train_sft/                # Extra: pipeline for supervised fine-tuning on STEM
â”‚   â”œâ”€â”€ train_mcqa.sh             # Training script for MCQA model
â”‚   â”œâ”€â”€ train_quantized.sh        # Training script for Quantized model
â”‚   â”œâ”€â”€ train_rag.sh              # Training script for RAG model
â”‚   â”œâ”€â”€ train_dpo.sh              # Training script for DPO model
â”‚   â””â”€â”€ train_sft.sh           # Optional script to run the SFT pipeline
â”‚
â”œâ”€â”€ model_configs/
â”‚   â”œâ”€â”€ mcqa_model.yaml
â”‚   â”œâ”€â”€ quantized_model.yaml
â”‚   â”œâ”€â”€ rag_model.yaml
â”‚   â””â”€â”€ dpo_model.yaml
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ data_repo.json            # Pointers to training datasets on Hugging Face Hub
â”‚
â”œâ”€â”€ pdf/
â”‚   â””â”€â”€ LiSoViMa.pdf          # Final project report
â”‚
â”œâ”€â”€ _templates/
â”‚   â””â”€â”€ mnlp_report_template.tex  # Report template used for the final project report
â”‚
â””â”€â”€ _test/
    â””â”€â”€ run_tests.py              # Scripts to validate submission format
```

---

## ðŸ”§ Training Scripts

Each model has its own training script that reproduces the fine-tuning process:

```bash
bash train_mcqa.sh
bash train_quantized.sh
bash train_rag.sh
bash train_dpo.sh
bash train_sft.sh
```

These scripts will:
- Construct the appropriate datasets
- Train the model using your specified configuration
- Save the final and best checkpoints locally

You should be able to reproduce the submission by running each script independently.

### `train_sft.sh`

This optional script trains a **base language model** on STEM-related multiple-choice QA data using a **Supervised Fine-Tuning (SFT)** approach. It is useful for pretraining a foundational model before applying more advanced techniques like MCQA, RAG or quantization.

The script performs the following:
 
- Select a subset of num_rows rows from source_dataset
- Pushes the subset dataset on Hugging Face Hub
- Fine-tunes base_model using the indicated arguments on the subset dataset
- Pushes the final checkpoint to the Hugging Face Hub

Usage: 
```bash
bash train_sft.sh
```

### `train_mcqa.sh`
This script fine-tunes the MCQA model on STEM-related multiple-choice QA data using the Low-Rank Adaptation technique. It leverages a pre-trained base model and adapts it for multiple-choice question answering tasks by incorporating LoRA to improve performance with fewer parameters.

The script performs the following steps:

- Installs the necessary dependencies from train_mcqa/requirements.txt.
- Prepares and preprocesses the MCQA dataset, or create and preprocess it if non-existant.
- Loads the base model (Qwen/Qwen3-0.6B-Base or given one) and tokenizer.
- Fine-tunes the base model using LoRA adapters, with specific hyperparameters (learning rate, batch size, gradient accumulation steps).
- Saves the final trained model to the specified output directory.

Usage: 
```bash
bash train_mcqa.sh
```

### `train_quantized.sh`
This script trains a quantized version of the model using 4-bit QLoRA fine-tuning on STEM-related multiple-choice QA data. It builds the dataset automatically if not found, and applies 4-bit quantization with LoRA adapters to optimize for memory efficiency without sacrificing accuracy.

The script performs the following steps:
-	Installs dependencies from train_quantized/quantized_requirements.txt
-	Prepares and preprocesses the MCQA dataset, or create and preprocess it if non-existant
-	Loads the Qwen3-0.6B-Base model in 4-bit (nf4, double quant, bfloat16)
-	Injects LoRA adapters into all linear layers of the quantized model
-	Fine-tunes the model on the MCQA dataset
-	Merges the LoRA adapters back into the base model
-	Saves the final trained model to the specified output directory

Usage: 
```bash
bash train_quantized.sh
```

### `train_rag.sh`

This script fine-tunes a **RAG-ready language model** on STEM-related multiple-choice QA data using **LoRA** and a **retrieval-augmented generation (RAG)** setup. It assumes a prior base model fine-tuned via SFT, and adds retrieval capabilities by aligning the model with an external corpus built from PDF textbooks.

The script performs the following steps:

- Installs dependencies from `train_rag/requirements.txt`
- Applies OCR (via Mistral API) to extract markdown from PDFs
- Splits the markdown content into tokenized chunks using a Hugging Face tokenizer
- Uploads the resulting RAG corpus to the Hugging Face Hub
- Builds a **FAISS index** on the RAG corpus using the specified embedding model and stores it locally in a uniquely named folder (based on key RAG parameters)
- Loads the base SFT model and applies a **LoRA adapter** using RAG-formatted MCQA data
- Fine-tunes the model and stores it locally in a uniquely named folder (based on key RAG parameters)
- Merging LoRA + base
- Stores the merged model 
- Stores the FAISS database locally in the same folder, enabling efficient evaluation
- Evaluates the merged model on several MCQA benchmarks

The **LoRA model**, **merged LoRA + base model** and the **retriever FAISS index** are saved in a uniquely named output folder (under `./LoRA/`, `./LoRA/merged/` and `./FAISS/`) based on:

- base model name
- RAG corpus name
- embedding model
- chunking parameters
- similarity function used

This setup ensures full reproducibility and allows evaluating or reusing the model without recomputation.

Usage:  
```bash
bash train_rag.sh
```

### `train_dpo.sh`
This script trains a causal language model using the Direct Preference Optimization (dpo) method. It performs the following steps: 

- Load `/source_dataset/` from Hugging Face Hub.
- Select only `\["prompt", "chosen", "rejected"]`\ columns.
- Split the data into 90% train and 10% validation using seed.
- Load a pretrained base model and tokenizer from Hugging Face.
- Configure DPO training using DPOConfig.
- Initialize the DPO trainer.
- Check if there is any checkpoint saved in the output directory and starts from the latest one. Otherwise, trains from scratch.
- Save final model and tokenizer to `\output_dir`\ named `\dpo_output`\.

Usage: 
```bash
bash train_dpo.sh
```
